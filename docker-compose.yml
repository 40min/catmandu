services:
  catmandu-core:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: catmandu-core
    environment:
      # Required environment variables
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}

      # Optional configuration with defaults
      - UPDATE_ID_FILE_PATH=${UPDATE_ID_FILE_PATH:-.data/update_id.txt}
      - MAX_MESSAGES_PER_CHAT=${MAX_MESSAGES_PER_CHAT:-100}
      - MAX_MESSAGE_LENGTH=${MAX_MESSAGE_LENGTH:-1000}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

      # Audio processing configuration
      - AUDIO_PROCESSING_ENABLED=${AUDIO_PROCESSING_ENABLED:-false}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - MAX_AUDIO_FILE_SIZE_MB=${MAX_AUDIO_FILE_SIZE_MB:-25}
      - MAX_AUDIO_DURATION_MINUTES=${MAX_AUDIO_DURATION_MINUTES:-10}
      - WHISPER_COST_PER_MINUTE=${WHISPER_COST_PER_MINUTE:-0.006}
      - GPT4O_MINI_INPUT_COST_PER_1M_TOKENS=${GPT4O_MINI_INPUT_COST_PER_1M_TOKENS:-0.15}
      - GPT4O_MINI_OUTPUT_COST_PER_1M_TOKENS=${GPT4O_MINI_OUTPUT_COST_PER_1M_TOKENS:-0.60}
      - COST_LOGS_DIR=${COST_LOGS_DIR:-logs/costs}

      # Python environment
      - PYTHONPATH=/app/src
      - UVICORN_HOST=0.0.0.0
      - UVICORN_PORT=8000
    ports:
      - "8000:8000"
    volumes:
      # Persist update ID file for Telegram polling offset
      - update_data:/app/.data
      # Persist chat logs
      - chat_logs:/app/logs
      # Persist cost logs for audio processing
      - cost_logs:/app/logs/costs
    depends_on:
      echo-cattackle:
        condition: service_healthy
    networks:
      - catmandu-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  echo-cattackle:
    build:
      context: ./cattackles/echo
      dockerfile: Dockerfile
      target: production
    container_name: echo-cattackle
    environment:
      # OpenAI API configuration (primary)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      # Gemini API configuration (fallback)
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-2.5-flash-lite-preview-06-17}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MCP_SERVER_PORT=${MCP_SERVER_PORT:-8001}
    expose:
      - "8001"
    networks:
      - catmandu-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  catmandu-network:
    driver: bridge
    name: catmandu-network

volumes:
  update_data:
    name: catmandu-update-data
  chat_logs:
    name: catmandu-chat-logs
  cost_logs:
    name: catmandu-cost-logs
